---
layout:     post
title:      "机器学习知识点"
subtitle:   " \"机器学习\""
date:       2018-12-06 10:00:00
author:     "lang"
header-img: "http://lyang-blog-pics.oss-cn-shanghai.aliyuncs.com/post-bg-2017/0330/170330.jpg"

catalog: true
tags:
    - Tech
---

# 线性回归

预测函数  
<center>$$y_i = \beta_0 + \beta_1x_{i1} + \dot + \beta_px_{ip} + \epsilon$$</center>  
其中$$\epsilon$$为噪声，可以用矩阵形式表示：  
<center>$$\mathbf{Y} = \mathbf{X}\beta + \epsilon$$</center>  
**最小二乘法**最小化误差的平方和来求解最佳预测函数  
<center>$$\text{min}\sum_{i=1}^{n}(y_i - \alpha - \beta x_i)^2$$</center>  

# 逻辑回归

逻辑回归就是在线性回归的基础上套用了一个逻辑函数：  
<center>$$y = \frac{1}{1+e^{-x}}$$</center>  
将预测值限定在[0, 1]之间，在x = 0时最敏感（变化最快），而对于线性回归，它的敏感度在整个实数域是一样的，因此噪声对线性回归的影响很大  
逻辑回归的预测函数  
<center>$$h_\theta(x) = g(\theta^Tx) = \frac{1}{1+e^{\theta^Tx}}, g(z) = \frac{1}{1+e^{-z}}$$</center>  
回归模型求解：  

1. 根据先验概率和条件概率求解后验概率  
<center>$$p(y \mid x, \theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}, y = 0, 1$$</center>  

2. 极大似然估计  
<center>$$L(\theta \mid x, y) = \prod_i p(y^{(i)} \mid x^{(i)}; \theta) = \prod_i (h_\theta(x))^{y^{(i)}}(1-h_\theta(x))^{1-y^{(i)}}$$</center>  

3. 对数似然估计  
<center>$$l(\theta) = \mathrm{log}(L(\theta)) = \sum_i y^{(i)}h_\theta(x) + (1-y^{(i)})(1- h_\theta(x))$$</center>  

4. 梯度下降求最优$$\theta$$, 即求$$\mathrm{argmin}(l(\theta))$$  
<center>$$\theta_j = \theta_j + \alpha(y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)}$$</center>  

5. 评估， 通常用AUC评估，AUC为ROC曲线下面积，表示正确判断的概率，当 1 > AUC > 0.5 则说明预测由于随机预测，当 AUC = 0.5时则说明预测跟随机预测一样， 当 0< AUC < 0.5时说明预测正确概率小于随机预测，但只要一直是反向预测那么也是优于随机预测的。  
ROC空间包含4个量：

* TP, 预测为患病，实际也患病了
* FP, 预测为患病，实际没患病
* TN, 预测没患病， 实际也没患病
* FN, 预测没患病，实际患病了

ROC坐标的x轴为$$TPR = \frac{TP}{TP+FN}$$, 即正确判断为患病的概率， y轴为$$FPR = \frac{FP}{FP+TN}$$,即正确判断没患病的概率  

# 决策树

* ID3, 信息增益
* C4.5, 信息增益比
* CART, 基尼指数

剪枝
 
* 预剪枝-在树生长的过程中设定阈值，当超过阈值则停止生长
* 后剪枝-先生成完整的树，再用测试数据验证并剪枝

# 随机森林

解决决策树过拟合问题  
Bagging策略，基于多数投票机制  
一般用于回归和分类  
基于多个决策树的分类器  
单颗树的生成过程：

1. N个样本， M个特征
2. 选取m个特征，m远小于M
3. 有放回的抽取N次（Bootstrap），未抽取到的数据作为测试数据，评估误差
4. 根据m个特征计算最佳分裂方式
5. 每棵树都会完整生长，不会剪枝

优点：

* 对于多维特征的数据准确度高
* 能够处理大量的数据
* 对于不平衡数据可以平衡误差
* 高效、快速
* 可以延伸到非监督聚类问题

# GBDT(Gradient Boosting Decision Tree)

又称MART(Multiple Additive Regression Tree)  
常用于回归、分类和排序
是一个迭代的决策树算法
基学习器都是**回归树**，这是因为迭代的过程是一个累加的过程，对于回归树的变量比如年龄10岁+2岁=12岁有实际意义，而分类树中比如性别男+性别女并没有实际意义  
GBDT的核心在于累加所有树的结果得到最终结果，而每棵树学习的是前一颗树的残差，即真实-预测  
之所以有Gradient，并不是这里面有通常的微分求导，而是无论cost function是什么，只要是以误差作为衡量标准，那么残差向量指向的就是全局最优方向  
Boosting在于，每步残差计算变相增大分错样本的权重，对分对的样本权重趋于0，使得后面的树能专注于分错的样本  
GBDT树的深度一般为6，而随机森林的树深度一般15或者更大，这是由误差来决定的，树的深度体现的是模型的复杂度，随着模型复杂度的提升，偏差（bias）会减小，而方差（variance）会增大，带来的就是过拟合问题，模型误差（error）呈U字形。当复杂度在一定值可以找到一个平衡点误差最小，设定树的深度就是在找这个平衡点。对于随机森林和GBDT来说，由于每个树都是随机独立的分布的，方差可以看成一样，在随机森林中树的学习过程实际就是最小化偏差，而GBDT在上一轮的基础上更加拟合数据，保证了偏差更小，因此所需要的深度更浅  
GBDT防止过拟合：加正则项，即迭代步长；调整子采样比率（不放回采样）；剪枝

# XGBoost

GBDT的进化版本，可并行，但是是基于特征粒度的并行，而不是Tree粒度的并行

# 特征选择

* Filter，对每个维度的特征进行打分（加权）和排序

	1. 卡方检验，$$X = \sum\frac{(A-T)^2}{T}$$,A为真实值，T为理论值
	2. 信息增益
	3. 相关系数

* Wrapper，随机生成不同的特征组合，对组合进行评价，找最优的组合，是一个最优解问题，常用粒子群、蚁群等算法
* Embedded，在模型既定下学习出最优组合，比如加$$L_1$$正则项,加入$$L_1$$后会得到系数的特征矩阵，是由于求最优cost function的过程中总是先到达$$L_1$$的顶点，顶点上总有一个维度为0

# 降维

特征选择一般是选出子集，而降维是通过属性间的关系，得到新的特征空间，并将数据映射到新的特征空间  
降维方法：

* 主成分分析（PCA）
* 奇异值分解(SVD)
* 映射

PCA:
  
1. 去平均值
2. 计算协方差矩阵
3. 计算特征值和特征向量
4. 对特征值进行排序
5. 保留前K个最大特征值的特征向量
6. 将数据转换到K个特征向量的新空间，相乘

# 激活函数

为什么要用激活函数：非线性、可微分、单调、输出范围有限

* Sigmoid，$$f(x) = \frac{1}{1+e^{-x}}$$
* tanh, $$tanh(x) = 2sigmoid(2x) - 1$$
* ReLU(线性整流), $$f(x) = max(0, x)$$

# 正则化

解决过拟合可以两种方案：

1. 尽量减少变量的数量
2. 减小变量数量的权值，这样一个变量变化就不会对整个模型有过大的影响，这就是正则化

常见：

1. $$L_0$$, 非零元素的个数
2. $$L_1$$，绝对值之和
3. $$L_2$$，平方和在开根

