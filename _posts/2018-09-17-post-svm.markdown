---
layout:     post
title:      "支持向量机SVM"
subtitle:   " \" 统计学习方法\""
date:       2018-09-17 10:00:00
author:     "lang"
header-img: "http://lyang-blog-pics.oss-cn-shanghai.aliyuncs.com/post-bg-2017/0330/170330.jpg"

catalog: true
tags:
    - Tech
---

# 模型

给定线性可分的训练数据集，通过**间隔最大化**学习到的超平面为  
<center>$$w^* x + b^*$$</center>  
以及相应的分类决策函数  
<center>$$f(x) = \mathrm{sign}(w^* x + b^*)$$</center>  
称为线性可分支持向量机

# 函数间隔和几何间隔

**函数间隔**  
<center>$$\hat{\gamma}_i = y_i(wx_i + b)$$</center>  
选择超平面仅有函数间隔是不够的，因为只要成比例的改变$$w$$和$$b$$，比如$$2w, 2b$$，此时超平面并没有改变，但是函数间隔却变成原来的2倍，因此我们需要对法向量$$w$$加以约束，比如 **规范化**，函数间隔就变成了 **几何间隔**  
点$$x_i$$与超平面$$(w, b)$$的几何间隔为  
<center>$$\gamma_i = y_i (\frac{w}{\parallel w \parallel}x_i + \frac{b}{\parallel w \parallel})$$</center>  
其中$$\parallel w \parallel$$是$$w$$的$$L_2$$范数。

# 最大间隔求超平面

约束最优化原始问题  
<center>$$\max_{w, b} \gamma$$</center>  
<center>$$\mathrm{s.t.} y_i (\frac{w}{\parallel w \parallel}x_i + \frac{b}{\parallel w \parallel}) \geq \gamma, i = 1, 2, \cdot, N$$</center>  
根据几何间隔和函数间隔的关系，问题可以改写为  
<center>$$\max_{w, b} \frac{\hat{\gamma}}{\parallel w \parallel}$$</center>  
<center>$$\mathrm{s.t.} y_i (wx_i + b) \geq \hat{\gamma}, i = 1, 2, \cdot, N$$</center>  
函数间隔$$\hat{\gamma}$$的取值不会影响最优化问题的解，这就可以取$$\hat{\gamma} = 1$$，并且最大化$$\frac{1}{\parallel w \parallel}$$等价于最小化$$\frac{1}{2}\parallel w \parallel^2$$，于是得到等价最优化问题  
<center>$$\min_{w, b} \frac{1}{2}\parallel w \parallel^2$$</center>  
<center>$$\mathrm{s.t.} y_i (wx_i + b) - 1 \geq 0, i = 1, 2, \cdot, N$$ </center> 

# 对偶算法

把求解线性可分SVM最优化问题作为原始问题，利用**拉格朗日对偶性**，通过求解对偶问题来得到原始问题的最优解  
首先，引入拉格朗日乘子$$\alpha_i \geq 0$$，构建拉格朗日函数：  
<center>$$L(w, b, \alpha) = \frac{1}{2}\parallel w \parallel^2 - \sum_{i = 1}^{N}\alpha_iy_i(wx_i + b) + \sum_{i=1}^{N}\alpha_i$$</center>  
根据拉格朗日对偶性，原始问题的队友问题就是极大极小问题  
<center>$$\max_{\alpha}\min_{w,b}L(w, b, \alpha)$$</center>  
问了得到对偶问题的解，先求$$L(w, b, \alpha)$$对$$(w, b)$$的极小值，再求对$$\alpha$$的极大值


# 训练线性不可分的数据-软间隔最大化

训练数据的线性不可分意味着有一些样本点$$(x_i, y_i)$$不能满足函数间隔大于等于1的约束条件，为了解决这个问题，对每个样本点引入一个**松弛变量**$$\xi_i \geq 0$$,使得其函数间隔加上松弛变量以后大于等于1，于是，约束条件变成  
<center>$$y_i(wx_i + b) \geq 1 - \xi_i$$</center>  
同时目标函数由原来的$$\frac{1}{2}\parallel w \parallel^2$$变成了  
<center>$$\frac{1}{2}\parallel w \parallel^2 + C\sum_{i = 1}^{N}\xi_i$$</center>  
这里，$$C > 0$$称为**惩罚参数**，一般由应用问题决定其大小，$$C$$值大时对误分类惩罚增大，值小时对误分类惩罚减小。这就是**软间隔最大化**

# 合页损失函数

线性SVM学习还有另外一种解释，就是最小化以下目标函数  
<center>$$\sum_{i=1}^{N}[1-y_i(wx_i + b)]_+ + \lambda\parallel w \parallel^2$$</center>  
其中  
<center>$$L(y(wx_i + b)) = [1-y_i(wx_i + b)]_+$$</center>  
称为**合页损失函数**，下标 + 表示取正值函数  
<center>$$[z]_+ = \left\{ \begin{array}{ll} z, \& z > 0 \\ 0, \& z \leq 0 \end{array}\right$$</center>  
